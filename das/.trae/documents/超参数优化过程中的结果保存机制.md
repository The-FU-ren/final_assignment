### 问题分析
当前代码中，设备设置为要么使用GPU要么使用CPU，无法同时高效协同工作。虽然理论上可以将模型的不同部分放在不同设备上，但由于GPU和CPU之间的数据传输开销很大，这种方式通常不会提高性能。

### 解决方案
更实际的做法是优化CPU和GPU的协同工作，让它们各司其职：

1. **GPU负责核心计算**：模型前向传播、反向传播和参数更新
2. **CPU负责辅助任务**：数据加载、预处理、后处理和结果保存
3. **异步数据加载**：在GPU进行模型训练的同时，CPU预加载下一批数据
4. **混合精度训练**：减少GPU内存使用，提高训练速度

### 修改内容

#### 1. 在 `train.py` 中优化数据加载
```python
# 使用num_workers参数，启用多线程数据加载
kfold_loaders = create_kfold_loaders(data, labels, batch_size=batch_size, snr_db=snr_db, num_workers=4)
```

#### 2. 修改 `data_processor.py` 中的 `create_kfold_loaders` 函数，添加 `num_workers` 参数
```python
def create_kfold_loaders(data, labels, batch_size=128, snr_db=0, n_splits=10, num_workers=4):
    """创建十折交叉验证的数据加载器"""
    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    loaders = []
    
    for train_idx, test_idx in kfold.split(data):
        # 划分训练集和测试集
        train_data, test_data = data[train_idx], data[test_idx]
        train_labels, test_labels = labels[train_idx], labels[test_idx]
        
        # 预处理
        train_data = preprocess_data(train_data, snr_db)
        test_data = preprocess_data(test_data, snr_db)
        
        # 创建数据集
        train_dataset = DAADataset(train_data, train_labels)
        test_dataset = DAADataset(test_data, test_labels)
        
        # 创建数据加载器，使用num_workers启用多线程
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)
        
        loaders.append((train_loader, test_loader))
    
    return loaders
```

#### 3. 添加混合精度训练支持
```python
# 在train.py中添加混合精度训练
from torch.cuda.amp import autocast, GradScaler

# 在train_model函数中初始化GradScaler
scaler = GradScaler()

# 修改train_one_epoch函数，添加混合精度训练
def train_one_epoch(model, train_loader, criterion, optimizer, device):
    """训练一个epoch"""
    model.train()
    running_loss = 0.0
    all_preds = []
    all_labels = []
    
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        
        optimizer.zero_grad()
        
        # 使用混合精度训练
        with autocast():
            outputs = model(inputs)
            loss = criterion(outputs, labels)
        
        # 缩放损失，反向传播
        scaler.scale(loss).backward()
        # 更新参数
        scaler.step(optimizer)
        # 更新缩放器
        scaler.update()
        
        running_loss += loss.item() * inputs.size(0)
        
        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
    
    epoch_loss = running_loss / len(train_loader.dataset)
    epoch_acc = accuracy_score(all_labels, all_preds)
    epoch_f1 = f1_score(all_labels, all_preds, average='macro')
    
    return epoch_loss, epoch_acc, epoch_f1
```

#### 4. 优化模型，减少GPU内存使用
```python
# 在train_model函数中，添加梯度累积
gradient_accumulation_steps = 2

# 修改训练循环，添加梯度累积
for epoch in range(epochs):
    for i, (inputs, labels) in enumerate(train_loader):
        inputs, labels = inputs.to(device), labels.to(device)
        
        optimizer.zero_grad()
        
        with autocast():
            outputs = model(inputs)
            loss = criterion(outputs, labels)
        
        # 梯度缩放和反向传播
        scaled_loss = loss / gradient_accumulation_steps
        scaler.scale(scaled_loss).backward()
        
        # 每gradient_accumulation_steps步更新一次参数
        if (i + 1) % gradient_accumulation_steps == 0:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
```

### 预期效果
1. **提高训练速度**：通过异步数据加载，GPU在训练的同时，CPU预加载下一批数据
2. **减少GPU内存使用**：通过混合精度训练和梯度累积，减少GPU内存占用
3. **更高效的协同工作**：CPU和GPU各司其职，充分发挥各自优势
4. **更好的资源利用率**：CPU和GPU都能保持较高的利用率

### 注意事项
1. **num_workers设置**：根据CPU核心数调整，通常设置为CPU核心数的2-4倍
2. **混合精度训练**：可能会影响模型精度，需要进行测试
3. **梯度累积**：会增加训练时间，但可以减少GPU内存使用
4. **数据加载优化**：确保数据加载和预处理在CPU上高效进行

这种优化方式比简单地同时使用CPU和GPU更实际，能真正提高训练效率。