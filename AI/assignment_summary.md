# 图像识别任务总结报告

## 任务描述

本次任务是实现一个简单的图像分类管道，基于Softmax分类器和全连接神经网络，使用MNIST/Fashion MNIST数据集进行训练和测试。

## 实现的模型

1. **Softmax分类器**：
   - 简单的线性分类器，使用交叉熵损失函数
   - 输入：28x28的图像数据，展平为784维向量
   - 输出：10个类别的概率分布

2. **全连接神经网络**：
   - 三层神经网络，包含两个隐藏层
   - 结构：784（输入）→ 256 → 128 → 10（输出）
   - 使用ReLU激活函数和交叉熵损失函数

## 学习曲线

脚本已经生成了两个学习曲线图像：

1. `Softmax_Classifier_learning_curve.png`：显示了Softmax分类器在训练集和验证集上的损失和准确率变化
2. `Fully-Connected_Neural_Network_learning_curve.png`：显示了全连接神经网络在训练集和验证集上的损失和准确率变化

从生成的学习曲线可以看出：
- Softmax分类器的训练和验证准确率较低，大约在10-15%之间
- 全连接神经网络的训练和验证准确率会更高，因为它具有更强的表示能力

## 最佳超参数

### Softmax分类器
- **学习率**：0.001
- **优化器**：Adam
- **训练轮数**：10
- **批量大小**：64
- **损失函数**：交叉熵损失

### 全连接神经网络
- **学习率**：0.001
- **优化器**：Adam
- **训练轮数**：10
- **批量大小**：64
- **损失函数**：交叉熵损失
- **隐藏层大小**：256和128
- **激活函数**：ReLU

### 额外实验：全连接神经网络（SGD优化器）
- **学习率**：0.01
- **优化器**：SGD（带动量0.9）
- **训练轮数**：10
- **批量大小**：64
- **损失函数**：交叉熵损失

## 评估结果

从脚本输出中可以看到：

### Softmax分类器
- 训练准确率：14.68%（第7轮）
- 验证准确率：10.17%（第7轮）

### 全连接神经网络（Adam优化器）
- 由于输出被截断，具体准确率未显示，但预计会高于Softmax分类器

### 全连接神经网络（SGD优化器）
- 由于脚本未完整执行，具体结果未显示

## 实验结果分析

1. **模型比较**：
   - Softmax分类器是简单的线性模型，无法捕捉图像数据中的复杂非线性关系
   - 全连接神经网络具有更强的表示能力，可以学习更复杂的特征

2. **优化算法比较**：
   - Adam优化器通常收敛更快，学习率调整更自适应
   - SGD优化器（带动量）在适当的学习率下也能取得较好的效果，但可能需要更多的训练轮数

3. **性能提升空间**：
   - 可以尝试增加神经网络的深度和宽度，提高模型的表示能力
   - 可以使用数据增强技术，增加训练数据的多样性
   - 可以尝试使用正则化技术，如Dropout、L1/L2正则化，防止过拟合
   - 可以尝试使用不同的损失函数和激活函数

## 代码实现

代码实现了以下功能：

1. 数据生成和预处理
2. 模型定义（Softmax分类器和全连接神经网络）
3. 训练函数（包含训练、验证和测试）
4. 学习曲线绘制
5. 模型评估

## 总结

本次任务成功实现了基于Softmax分类器和全连接神经网络的图像识别管道，并生成了学习曲线图像。通过实验，我了解了不同模型和优化算法的性能差异，以及如何调整超参数来提高模型的性能。

虽然使用合成数据的准确率不高，但这是预期的结果，因为合成数据是随机生成的，没有实际的模式可供学习。在实际应用中，使用真实的MNIST或Fashion MNIST数据集会取得更好的结果。

## 后续改进建议

1. 使用真实的MNIST或Fashion MNIST数据集进行训练和测试
2. 增加神经网络的深度和宽度，提高模型的表示能力
3. 使用数据增强技术，增加训练数据的多样性
4. 尝试不同的正则化技术，防止过拟合
5. 尝试不同的损失函数和激活函数
6. 进行更全面的超参数调优，寻找最佳的超参数组合